{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Defining imports"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np \n","import pandas as pd\n","import pickle\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.svm import SVR\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler, normalize\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error"]},{"cell_type":"markdown","metadata":{},"source":["### Loading the dataset\n","\n","After dataset loading, we remove duplicated or NaN rows.\n","\n","52 rows are identified as duplicate. No NaN is found."]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["252175\n","252123\n","252123\n"]}],"source":["path = \"../datasets/train.csv\"\n","df = pd.read_csv(path)\n","\n","print(len(df))\n","\n","df = df.drop_duplicates()\n","\n","print(len(df))\n","\n","df = df.dropna()\n","\n","print(len(df))"]},{"cell_type":"markdown","metadata":{},"source":["******"]},{"cell_type":"markdown","metadata":{},"source":["### Dataset splitting"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[],"source":["X = df.drop('Year', axis=1)\n","y = df['Year']"]},{"cell_type":"markdown","metadata":{},"source":["### Outlier removal via winsorization\n","When cell values are respectively below or above the 5th lower or upper percentile, we change its value to the treshold."]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["def winsorize_outliers(df, column, lower_limit, upper_limit):\n","    df[column] = np.where(df[column] < lower_limit, lower_limit, df[column])\n","    df[column] = np.where(df[column] > upper_limit, upper_limit, df[column])\n","    return df\n","\n","lower_limit = X.quantile(0.05, axis=0)\n","upper_limit = X.quantile(0.95, axis=0)\n","\n","for col in X.columns:\n","    X = winsorize_outliers(X.copy(), col, lower_limit[col], upper_limit[col])"]},{"cell_type":"markdown","metadata":{},"source":["### Dataset splitting\n","We split both `X` and `y` into training and validation sets, before converting them back into dataframes"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["----Training Set----\n","X_train Shape: (201698, 90)\n","y_train Shape: (201698, 1)\n","\n","----Test Set----\n","X_test Shape: (50425, 90)\n","y_test Shape: (50425, 1)\n"]}],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","X_train = pd.DataFrame(X_train)\n","y_train = pd.DataFrame(y_train)\n","\n","X_test = pd.DataFrame(X_test)\n","y_test = pd.DataFrame(y_test)\n","\n","print(\"----Training Set----\")\n","print(\"X_train Shape:\", X_train.shape)\n","print(\"y_train Shape:\", y_train.shape)\n","\n","print(\"\\n----Test Set----\")\n","print(\"X_test Shape:\", X_test.shape)\n","print(\"y_test Shape:\", y_test.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### MinMax Scaling\n","A MinMax scaler is used and saved for later usage. The StandardScaler and L1/L2 normalization were also tried but gave worse results on the 4 models."]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["scaler = MinMaxScaler()\n","\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","#normalization l1\n","#X_train = normalize(X_train, norm=\"l1\")\n","#X_test = normalize(X_test, norm=\"l1\")\n","\n","#normalization l2\n","#X_train = normalize(X_train, norm=\"l2\")\n","#X_test = normalize(X_test, norm=\"l2\")\n","\n","#standardization\n","#scalerStandard = StandardScaler()\n","\n","#X_train = scalerStandard.fit_transform(X_train)\n","#X_test = scalerStandard.transform(X_test)\n","\n","file = open(\"scaler.save\",\"wb\")\n","pickle.dump(scaler, file)\n","file.close()"]},{"cell_type":"markdown","metadata":{},"source":["### PCA\n","A principal component analysis is saved in `pca.save` file. A variance of 99% was used, if decreased it would have a heavy impact on the results. In this way, however, the results remain approximately unchanged compared to the approach without PCA but the dimensionality of the dataset is slightly reduced."]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[],"source":["pca = PCA(0.99)\n","\n","X_train = pca.fit_transform(X_train)\n","X_test = pca.transform(X_test)\n","\n","file = open(\"pca.save\",\"wb\")\n","pickle.dump(pca, file)\n","file.close()"]},{"cell_type":"markdown","metadata":{},"source":["*****"]},{"cell_type":"markdown","metadata":{},"source":["### Machine Learning\n","We will use four models:\n","* Linear Regression\n","* K-Neighbours Regression\n","* Support Vector Regression\n","* Random Forest\n","\n","For each one of them we show performances on validation set."]},{"cell_type":"markdown","metadata":{},"source":["### Linear Regression\n","A linear regression model is instantiated, trained on training data and tested on testing data. Finally, the metrics are calculated."]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'model': 'lr', 'mse': 81.41203699814004, 'mae': 6.524166889778804, 'mape': 0.003274426855344165, 'r2score': 0.2530491834152526}\n"]}],"source":["linear_reg = LinearRegression()\n","\n","linear_reg.fit(X_train, y_train)\n","\n","y_pred_linear = linear_reg.predict(X_test)\n","\n","mse = mean_squared_error(y_test, y_pred_linear)\n","r2 = r2_score(y_test, y_pred_linear)\n","mae = mean_absolute_error(y_test, y_pred_linear)\n","mape = mean_absolute_percentage_error(y_test, y_pred_linear)\n","\n","perf = {\"model\": \"lr\", \"mse\": mse, \"mae\": mae, \"mape\": mape, \"r2score\": r2}\n","print(perf)\n","\n","file = open(\"lr.save\",\"wb\")\n","pickle.dump(linear_reg, file)\n","file.close()"]},{"cell_type":"markdown","metadata":{},"source":["### K-Neighbours Regressor\n","We implemented a `GridSearch` with various parameters.\n","\n","After the training, we pick the best performing model, we print metrics and we save it in `knr.save`\n","\n","The best performances were obtained with\n","* 30 neighbors\n","* `distance` weighting method\n","* p = 1"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 3 folds for each of 28 candidates, totalling 84 fits\n","Migliori parametri: {'n_neighbors': 30, 'p': 1, 'weights': 'distance'}\n","{'model': 'knr', 'mse': 81.36319668833968, 'mae': 6.656362605061602, 'mape': 0.0033404550622397416, 'r2score': 0.25349729048433944}\n"]}],"source":["knn = KNeighborsRegressor()\n","\n","param_grid_knn = {\n","    'n_neighbors': [1, 5, 10, 15, 20, 30, 50],  \n","    'weights': ['uniform', 'distance'],  \n","    'p': [1,2]\n","}\n","\n","grid_search_knn = GridSearchCV(estimator=knn, param_grid=param_grid_knn, refit=True, cv=3, scoring='neg_mean_squared_error', verbose=3, n_jobs=-1)\n","\n","grid_search_knn.fit(X_train, y_train)\n","\n","best_knn = grid_search_knn.best_estimator_\n","\n","print(\"Migliori parametri:\", grid_search_knn.best_params_)\n","\n","y_pred_knn = best_knn.predict(X_test)\n","\n","mse = mean_squared_error(y_test, y_pred_knn)\n","r2 = r2_score(y_test, y_pred_knn)\n","mae = mean_absolute_error(y_test, y_pred_knn)\n","mape = mean_absolute_percentage_error(y_test, y_pred_knn)\n","\n","perf = {\"model\": \"knr\", \"mse\": mse, \"mae\": mae, \"mape\": mape, \"r2score\": r2}\n","print(perf)\n","\n","file = open(\"knr.save\",\"wb\")\n","pickle.dump(best_knn, file)\n","file.close()"]},{"cell_type":"markdown","metadata":{},"source":["### Random Forest Regressor\n","\n","We also used `GridSearch` for RF model.\n","\n","Fewer configurations were tried at once due to longer execution times.\n","\n","As above, the model with the best performances is stored in a file named `rf.save`\n","\n","The best performances were obtained with\n","* 500 estimators\n","* 100 maximum depth"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 3 folds for each of 9 candidates, totalling 27 fits\n","Migliori parametri: {'max_depth': 100, 'n_estimators': 500}\n","{'model': 'rf', 'mse': 84.76421254108078, 'mae': 6.849759762022807, 'mape': 0.0034368472893292687, 'r2score': 0.22229316315755754}\n"]}],"source":["rf = RandomForestRegressor()\n","\n","param_grid = {\n","    'n_estimators': [100, 300, 500],  \n","    'max_depth': [None, 50, 100],  \n","    #'min_samples_split': [2, 5], #too much time\n","    #'min_samples_leaf': [1, 2],  #too much time\n","}\n","\n","grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3, refit=True, verbose=3, n_jobs=-1)\n","\n","grid_search_rf.fit(X_train, y_train.to_numpy().ravel())\n","\n","best_rf = grid_search_rf.best_estimator_\n","\n","print(\"Migliori parametri:\", grid_search_rf.best_params_)\n","\n","y_pred_rf = best_rf.predict(X_test)\n","\n","mse = mean_squared_error(y_test, y_pred_rf)\n","r2 = r2_score(y_test, y_pred_rf)\n","mae = mean_absolute_error(y_test, y_pred_rf)\n","mape = mean_absolute_percentage_error(y_test, y_pred_rf)\n","\n","perf = {\"model\": \"rf\", \"mse\": mse, \"mae\": mae, \"mape\": mape, \"r2score\": r2}\n","print(perf)\n","\n","file = open(\"rf.save\",\"wb\")\n","pickle.dump(best_rf, file)\n","file.close()"]},{"cell_type":"markdown","metadata":{},"source":["### Support Vector Regressor\n","This last training also contains a `GridSearch`:\n","\n","The chosen parameters were:\n","* `C=10`\n","* Radial basis function kernel"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 3 folds for each of 6 candidates, totalling 18 fits\n","Migliori parametri: {'C': 10, 'kernel': 'rbf'}\n","{'model': 'svr', 'mse': 72.59192773264266, 'mae': 5.679693741821042, 'mape': 0.0028536365200389907, 'r2score': 0.33397318508813145}\n"]}],"source":["svm = SVR()\n","\n","param_grid_svm = {\n","    'kernel': ['linear', 'rbf'], #poly too much time\n","    'C': [0.1, 1, 10], #0.01 too much time\n","}\n","\n","grid_search_svm = GridSearchCV(estimator=svm, param_grid=param_grid_svm, scoring='neg_mean_squared_error', cv=3, refit=True, verbose=3, n_jobs=-1)\n","\n","grid_search_svm.fit(X_train, y_train.to_numpy().ravel())\n","\n","best_svr = grid_search_svm.best_estimator_\n","\n","print(\"Migliori parametri:\", grid_search_svm.best_params_)\n","\n","y_pred_svm = best_svr.predict(X_test)\n","\n","mse = mean_squared_error(y_test, y_pred_svm)\n","r2 = r2_score(y_test, y_pred_svm)\n","mae = mean_absolute_error(y_test, y_pred_svm)\n","mape = mean_absolute_percentage_error(y_test, y_pred_svm)\n","\n","perf = {\"model\": \"svr\", \"mse\": mse, \"mae\": mae, \"mape\": mape, \"r2score\": r2}\n","print(perf)\n","\n","file = open(\"svr.save\",\"wb\")\n","pickle.dump(best_svr, file)\n","file.close()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4739161,"sourceId":8038612,"sourceType":"datasetVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
